# -*- coding: utf-8 -*-
"""cil_main_0706

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hgc5GdTc3RqeRM2UfZ515emfIRcMwvo4
"""

# ONLY RUN ONCE TO SETUP THE ENVIRONMENT
from getpass import getpass
import urllib
import os
import shutil
from random import sample
from glob import glob

!ls

# cleanup previous runs
!rm -r CIL-street
#!rm -r CIL_street

# load data from github repo
user = input('User name: ')
password = getpass('Password: ')
password = urllib.parse.quote(password) # your password is converted into url format
repo_name = 'CIL-street'
branch_name = 'han'
cmd_string = 'git clone --single-branch --branch {4} https://{0}:{1}@github.com/{2}/{3}.git'.format(user, password, 'JonathanLehner', repo_name, branch_name)
os.system(cmd_string)
cmd_string, password = "", "" # removing the password from the variable

!ls

if os.path.exists('CIL_street'):
    shutil.rmtree('CIL_street')
os.rename('CIL-street', 'CIL_street')
#import shutil
#shutil.move("CIL-street", "CIL_street")

!ls

# create separate validation folder
os.mkdir("CIL_street/data/validation")
os.mkdir("CIL_street/data/validation/images")
os.mkdir("CIL_street/data/validation/groundtruth")

# use 10 samples for validation
for img in sample(glob("CIL_street/data/training/images/*.png"), 10):
  os.rename(img, img.replace('training', 'validation'))
  mask = img.replace('images', 'groundtruth')
  os.rename(mask, mask.replace('training', 'validation'))

!pwd
!ls
!ls CIL_street

!pwd
!ls

!ls CIL_street

!cat CIL_street/README.md

from glob import glob
import IPython.display as display
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import datetime, os
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import tensorflow_addons as tfa

from CIL_street.code.preprocessing import *
from CIL_street.code.utils import *
from CIL_street.code.models import *
from CIL_street.code.loss import *
from CIL_street.code.metrics import *


# automatically chooses parameters
AUTOTUNE = tf.data.experimental.AUTOTUNE
print(f"Tensorflow ver. {tf.__version__}")
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        print(e)

!ls
!ls CIL_street
!ls CIL_street/data
!ls CIL_street/data/training
!ls CIL_street/data/training/images

SEED = 42
dataset_path = "CIL_street/"
training_data = "data/training/images/"
val_data = "data/validation/images/"

IMG_SIZE = 400
N_CHANNELS = 3
N_CLASSES = 2
BATCH_SIZE = 16  # CAREFUL! Batch size must be less than validation dataset size
BUFFER_SIZE = 1000
TRAINSET_SIZE = len(glob(dataset_path + training_data + "*.png"))
VALSET_SIZE = len(glob(dataset_path + val_data + "*.png"))
print(f"The Training Dataset contains {TRAINSET_SIZE} images.")
print(f"The Validation Dataset contains {VALSET_SIZE} images.")

# initialize datasets
train_dataset = tf.data.Dataset.list_files(dataset_path + training_data + "*.png", seed=SEED)
train_dataset = train_dataset.map(parse_image)
val_dataset = tf.data.Dataset.list_files(dataset_path + val_data + "*.png", seed=SEED)
val_dataset = val_dataset.map(parse_image)

print(train_dataset)
count=0
for element in train_dataset.as_numpy_iterator():
  count+=1
print(count)

print(val_dataset)
count=0
for element in val_dataset.as_numpy_iterator():
  count+=1
print(count)

# load the dataset and preprocess
dataset = {"train": train_dataset, "val": val_dataset}

tmp_func1 = get_load_image_train()
dataset['train'] = dataset['train'].map(tmp_func1, num_parallel_calls=tf.data.experimental.AUTOTUNE)
dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)
dataset['train'] = dataset['train'].repeat()
dataset['train'] = dataset['train'].batch(BATCH_SIZE)
dataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)

print(tmp_func1)
print(get_load_image_train())

tmp_func2 = get_load_image_test()
dataset['val'] = dataset['val'].map(tmp_func2)
dataset['val'] = dataset['val'].repeat()
dataset['val'] = dataset['val'].batch(BATCH_SIZE)
dataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)

# display a single sample
for image, mask in dataset['train'].take(1):
    sample_image, sample_mask = image, mask
print('Sample from training dataset')
display_sample([sample_image[0], sample_mask[0]])

# create model
model = UNet()
# model.summary()
model.compile(optimizer=Adam(0.0001), loss = 'binary_crossentropy', metrics=['accuracy', kaggle_metric])
print('Predictions by untrained model')
show_predictions(model, dataset['train'])

EPOCHS = 20
STEPS_PER_EPOCH = max(TRAINSET_SIZE // BATCH_SIZE, 1)
VALIDATION_STEPS = max(VALSET_SIZE // BATCH_SIZE, 1)

# prepare callbacks
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

class DisplayCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        clear_output(wait=True)
        show_predictions(model, dataset['val'])
        print ('\nSample Prediction after epoch {}\n'.format(epoch+1))

callbacks = [
    # to show samples after each epoch
    DisplayCallback(),
    # tensorboard_callback,
    # tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),
    # tf.keras.callbacks.ModelCheckpoint('best_model_unet.h5', verbose=1, save_best_only=True, save_weights_only=True)
]

# begin training
model_history = model.fit(dataset['train'], epochs=EPOCHS,
                          steps_per_epoch=STEPS_PER_EPOCH,
                          validation_steps=VALIDATION_STEPS,
                          validation_data=dataset['val'],
                          callbacks=callbacks)

plot_loss(model_history)
!mkdir CIL_street/data/output
save_predictions(model)
!zip -r CIL_street/output.zip CIL_street/data/output

